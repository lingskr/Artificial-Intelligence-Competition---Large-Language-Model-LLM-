{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5632975,"sourceType":"datasetVersion","datasetId":3238926},{"sourceId":6572938,"sourceType":"datasetVersion","datasetId":3600418},{"sourceId":7714580,"sourceType":"datasetVersion","datasetId":4505324},{"sourceId":7893017,"sourceType":"datasetVersion","datasetId":4634330},{"sourceId":8112383,"sourceType":"datasetVersion","datasetId":4792325},{"sourceId":164964691,"sourceType":"kernelVersion"},{"sourceId":170574075,"sourceType":"kernelVersion"},{"sourceId":10716,"sourceType":"modelInstanceVersion","modelInstanceId":8658},{"sourceId":21555,"sourceType":"modelInstanceVersion","modelInstanceId":17852}],"dockerImageVersionId":30665,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#https://www.kaggle.com/datasets/tomokihirose/faiss-gpu-173-python310\n#https://www.kaggle.com/datasets/datafan07/llm-whls\n#https://www.kaggle.com/datasets/ahmadsaladin/mistral-7b-it-v02\n#https://www.kaggle.com/models/Microsoft/phi/Transformers/2/1\n#https://www.kaggle.com/models/mozhiwenmzw/phi2-public-data-sft-adapter/PyTorch/public-data-sft/1\n#https://www.kaggle.com/code/levantaokkz/library-off-for-llm","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip install -U /kaggle/input/faiss-gpu-173-python310/faiss_gpu-1.7.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl\n!pip install -Uq /kaggle/input/llm-whls/bitsandbytes-0.41.1-py3-none-any.whl\n!pip install -Uq /kaggle/input/llm-whls/peft-0.4.0-py3-none-any.whl\n!pip install -Uq /kaggle/input/library-off-for-llm/transformers-4.38.2-py3-none-any.whl\n!pip install sentence-transformers","metadata":{"execution":{"iopub.status.busy":"2024-04-10T01:34:35.730615Z","iopub.execute_input":"2024-04-10T01:34:35.73101Z","iopub.status.idle":"2024-04-10T01:35:12.627835Z","shell.execute_reply.started":"2024-04-10T01:34:35.730971Z","shell.execute_reply":"2024-04-10T01:35:12.626509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 方案总揽\n* 1.seq2seq模型  \n* 2.开源微调模型phi2\n* 3.zero-shot大模型mistral-7b-v2\n\n集成三种模型的预测结果为最终结果","metadata":{}},{"cell_type":"markdown","source":"### 预处理\n生成训练集和验证集的embedding，便于加速训练","metadata":{}},{"cell_type":"code","source":"%%writefile trian_embedding_generate.py\nimport pandas as pd\nimport gc\nimport numpy as np\ndf = pd.read_parquet(f\"./train_clean.parquet\", columns=['rewrite_prompt'])\nvalid = pd.read_csv('./validation826.csv', usecols=['rewrite_prompt'])\n\nimport pandas as pd\nimport time\nfrom tqdm import tqdm\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nimport pickle\nmodel =  SentenceTransformer('sentence-transformers/sentence-t5-base')#\nmodel.max_seq_length = 512\nencoded_data = model.encode(list(df['rewrite_prompt']), batch_size=64, device='cuda', show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nencoded_data = encoded_data.detach().cpu().numpy()\nencoded_data = np.asarray(encoded_data.astype('float32'))\n\nnp.save('train_clean_emb_sentence-t5-base.npy', encoded_data)\n\nvalid_emb = model.encode(list(valid['rewrite_prompt']), batch_size=64, device='cuda', show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nvalid_emb = valid_emb.detach().cpu().numpy()\nvalid_emb = np.asarray(valid_emb.astype('float32'))\n\nnp.save('valid826_emb_sentence-t5-base.npy', valid_emb)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### seq2seq训练","metadata":{}},{"cell_type":"code","source":"%%writefile seq2seq_exp14_train.py\nimport gc\nimport math\nimport matplotlib.pyplot as plt\nimport multiprocessing\nimport numpy as np\nimport os\nimport pandas as pd\nimport random\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport uuid\n\nfrom glob import glob\nfrom torch.nn import Parameter\nfrom torch.optim import Adam, SGD, AdamW\nfrom torch.optim.lr_scheduler import OneCycleLR\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nfrom typing import Dict, List\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Current device is: {device}\")\n\nos.environ['TOKENIZERS_PARALLELISM']='true'\nimport tokenizers\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\nprint(f\"tokenizers.__version__: {tokenizers.__version__}\")\nprint(f\"transformers.__version__: {transformers.__version__}\")\n\nclass config:\n    AMP = True\n    BATCH_SIZE_TRAIN = 32 #若出现oom，减少即可\n    BATCH_SIZE_VALID = 32 #若出现oom，减少即可\n    BETAS = (0.9, 0.999)\n    DEBUG = 0 #debug改为1\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    LR = 5e-6\n    EPOCHS = 6\n    EPS = 1e-6\n    GRADIENT_CHECKPOINTING = False\n    MODEL = \"/kaggle/input/deberta-v3-large-hf-weights\" #模型文件-https://www.kaggle.com/datasets/radek1/deberta-v3-large-hf-weights\n    CKPT = 'deberta-v3-large'\n    MAX_GRAD_NORM = 100000.0\n    MAX_LEN = 384\n    NUM_WORKERS = 0\n    PRINT_FREQ = 500\n    SEED = 20\n    WANDB = False\n    WEIGHT_DECAY = 0.008\n\nclass paths:\n    TRAIN_DATA = \"./train_clean.parquet\"\n    #TRAIN_DATA2 = './train_sft_v13.csv'\n    VALID_DATA = './validation826.csv'\n    train_embedding_file = './train_clean_emb_sentence-t5-base.npy'\n    #train_embedding_file2 = './train_sft_v13_emb_sentence-t5-base.npy'\n    valid_embedding_file = './valid826_emb_sentence-t5-base.npy'\n    OUTPUT_DIR = \"./exp14\"#保存文件夹\n    LOGGER = 'exp14'\n\nos.makedirs(paths.OUTPUT_DIR, exist_ok=True)\n\nclass AverageMeter(object):\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum / self.count\n\n\ndef asMinutes(s):\n    m = math.floor(s / 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s / (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n\n\ndef get_config_dict(config):\n    config_dict = dict((key, value) for key, value in config.__dict__.items()\n    if not callable(value) and not key.startswith('__'))\n    return config_dict\n\n\ndef get_optimizer_params(model, encoder_lr, decoder_lr, weight_decay=0.0):\n    param_optimizer = list(model.named_parameters())\n    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n    optimizer_parameters = [\n        {'params': [p for n, p in model.model.named_parameters() if not any(nd in n for nd in no_decay)],\n         'lr': encoder_lr, 'weight_decay': weight_decay},\n        {'params': [p for n, p in model.model.named_parameters() if any(nd in n for nd in no_decay)],\n         'lr': encoder_lr, 'weight_decay': 0.0},\n        {'params': [p for n, p in model.named_parameters() if \"model\" not in n],\n         'lr': decoder_lr, 'weight_decay': 0.0}\n    ]\n    return optimizer_parameters\n\n\ndef get_logger(filename=paths.OUTPUT_DIR+'/'+paths.LOGGER):\n    from logging import getLogger, INFO, StreamHandler, FileHandler, Formatter\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=f\"{filename}.log\")\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\n\ndef seed_everything(seed=20):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\n\ndef generate_uuid():\n    return str(uuid.uuid4())\n\n\ndef to_device(inputs, device: str = device):\n    return {k: v.to(device) for k, v in inputs.items()}\n\nLOGGER = get_logger()\nseed_everything(seed=config.SEED)\n\ntokenizer = AutoTokenizer.from_pretrained(config.MODEL)\ntokenizer.save_pretrained(paths.OUTPUT_DIR + '/tokenizer/')\n\ndef prepare_input(cfg: type, text: np.ndarray, tokenizer):\n\n    inputs = tokenizer.encode_plus(\n        text,\n        return_tensors=None,\n        add_special_tokens=True,\n        max_length=cfg.MAX_LEN,\n        padding='max_length', # TODO: check padding to max sequence in batch\n        truncation=True\n    )\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long) # TODO: check dtypes\n    return inputs\n\n\ndef collate(inputs):\n    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max()) # Get batch's max sequence length\n    for k, v in inputs.items():\n        inputs[k] = inputs[k][:,:mask_len]\n    return inputs\n\n\nclass CustomDataset(Dataset):\n    def __init__(self, cfg, df, tokenizer, rewrite_prompts_embeddings):\n        self.cfg = cfg\n        self.original_texts = df['original_text'].fillna('').map(str).values\n        self.rewritten_texts = df['rewritten_text'].fillna('').map(str).values\n        self.rewrite_prompts = rewrite_prompts_embeddings\n        self.text_ids = df['id'].astype(str).values\n        self.tokenizer = tokenizer\n\n    def __len__(self):\n        return len(self.text_ids)\n\n    def __getitem__(self, item):\n        output = {}\n        output[\"original_text\"] = prepare_input(self.cfg, self.original_texts[item], self.tokenizer)\n        output[\"rewritten_text\"] = prepare_input(self.cfg, self.rewritten_texts[item], self.tokenizer)\n        output[\"rewrite_prompt\"] = self.rewrite_prompts[item]\n        output[\"id\"] = self.text_ids[item]\n        return output\n\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n\n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n\n\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, mode: str =\"train\", pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        self.mode = mode\n        self.dropout = 0.2\n        # Load config by inferencing it from the model name.\n        if config_path is None:\n            self.config = AutoConfig.from_pretrained(cfg.MODEL, output_hidden_states=True)\n            self.config.hidden_dropout = 0.\n            self.config.hidden_dropout_prob = 0.\n            self.config.attention_dropout = 0.\n            self.config.attention_probs_dropout_prob = 0.\n        # Load config from a file.\n        else:\n            self.config = torch.load(config_path)\n\n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.MODEL, config=self.config)\n        else:\n            self.model = AutoModel(self.config)\n\n        if self.cfg.GRADIENT_CHECKPOINTING:\n            self.model.gradient_checkpointing_enable()\n\n\n        self.pool = MeanPooling()\n        self.head = nn.Sequential(\n            nn.Linear(self.config.hidden_size*4, 32768),\n            nn.BatchNorm1d(32768),\n            nn.ReLU(),\n            nn.Linear(32768, 768),\n        )\n        self._init_weights(self.head)\n\n    def _init_weights(self, module):\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n\n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        #last_hidden_states = outputs[1]\n        feature1 = self.pool(outputs.hidden_states[-1], inputs['attention_mask'])\n        feature2 = self.pool(outputs.hidden_states[-2], inputs['attention_mask'])\n        return torch.cat([feature1, feature2], dim=1)\n\n    def forward(self, original_texts, rewritten_texts, rewrite_prompts_embedding):\n\n        original_texts_feature = self.feature(original_texts) # shape (batch_size, 768)\n        rewritten_texts_feature = self.feature(rewritten_texts) # shape (batch_size, 768)\n        feature = torch.cat([original_texts_feature, rewritten_texts_feature], dim=1) # shape (batch_size, 768 * 2)\n        output = self.head(feature)\n\n        if self.mode == \"train\":\n            prompt_embedding = torch.tensor(rewrite_prompts_embedding, device=self.cfg.DEVICE) # shape (batch_size, 768)\n        else:\n            prompt_embedding = None\n\n        return output, prompt_embedding\n\n\nmodel = CustomModel(config, config_path=None, pretrained=True)\ntotal_params = sum(p.numel() for p in model.parameters())\nprint(f\"Total number of parameters: {round(total_params/1e6, 2)} M\")\n\ndef sharpened_cosine_similarity(k: np.ndarray, s: np.ndarray, p: int = 3, q: float = 1e-10):\n    dot_product = np.dot(k, s)\n    norm_k = np.linalg.norm(k)\n    norm_s = np.linalg.norm(s)\n    # Compute the cosine similarity with added term q for numerical stability\n    cosine_similarity = dot_product / (norm_k * (norm_s + q))\n    # Compute the sharpened cosine similarity\n    score = np.sign(dot_product) * (cosine_similarity ** p)\n    return score\n\n\ndef scs(k_batch: np.ndarray, s_batch: np.ndarray, p: int = 3, q: float = 1e-10):\n    bs = k_batch.shape[0]\n    scores = []\n    for item in range(bs):\n        k = k_batch[item]\n        s = s_batch[item]\n        scores.append(sharpened_cosine_similarity(k, s, p, q))\n    score = np.mean(scores)\n    return score\n\n# Example usage\nk_vector = torch.tensor([[0.90, 0.10, 0.95], [0.05, 0.10, 0.99]])\ns_vector = torch.tensor([[1, 0, 1], [0, 0, 1]])\np_value = 3\n\nresult = scs(k_vector, s_vector)\nprint(\"Mean Sharpened Cosine Similarity:\", result)\n\ndef train_epoch(train_loader, model, criterion, optimizer, epoch, scheduler, device):\n    model.train() # set model in train mode\n    scaler = torch.cuda.amp.GradScaler(enabled=config.AMP) # Automatic Mixed Precision tries to match each op to its appropriate datatype.\n    losses = AverageMeter() # initiate AverageMeter to track the loss.\n    start = end = time.time() # track the execution time.\n    global_step = 0\n\n    # ========== ITERATE OVER TRAIN BATCHES ============\n    with tqdm(train_loader, unit=\"train_batch\", desc='Train') as tqdm_train_loader:\n        for step, batch in enumerate(tqdm_train_loader):\n            ids_batch = batch.pop(\"id\")\n            original_texts = to_device(collate(batch.pop(\"original_text\")))\n            rewritten_texts = to_device(collate(batch.pop(\"rewritten_text\")))\n            rewrite_prompts = batch.pop(\"rewrite_prompt\")\n            batch_size = len(ids_batch)\n            targets = torch.ones(batch_size, device=device) # -1 for dissimilar, 1 for similar\n            with torch.cuda.amp.autocast(enabled=config.AMP):\n                y_preds, y_trues = model(original_texts, rewritten_texts, rewrite_prompts) # forward propagation pass\n                loss = criterion(y_preds, y_trues, targets) # get loss\n            losses.update(loss.item(), batch_size) # update loss function tracking\n            scaler.scale(loss).backward() # backward propagation pass\n            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), config.MAX_GRAD_NORM)\n            scaler.step(optimizer) # update optimizer parameters\n            scaler.update()\n            optimizer.zero_grad() # zero out the gradients\n            global_step += 1\n            scheduler.step() # update learning rate\n            end = time.time() # get finish time\n\n            # ========== LOG INFO ==========\n            if step % config.PRINT_FREQ == 0 or step == (len(train_loader)-1):\n                print('Epoch: [{0}][{1}/{2}] '\n                      'Elapsed {remain:s} '\n                      'Loss: {loss.avg:.4f} '\n                      'Grad: {grad_norm:.4f}  '\n                      'LR: {lr:.8f}  '\n                      .format(epoch+1, step, len(train_loader),\n                              remain=timeSince(start, float(step+1)/len(train_loader)),\n                              loss=losses,\n                              grad_norm=grad_norm,\n                              lr=scheduler.get_lr()[0]))\n            if config.WANDB:\n                wandb.log({f\"[fold_{fold}] train loss\": losses.val,\n                           f\"[fold_{fold}] lr\": scheduler.get_lr()[0]})\n\n    gc.collect()\n\n    return losses.avg\n\n\ndef valid_epoch(valid_loader, model, criterion, device):\n    model.eval() # set model in evaluation mode\n    losses = AverageMeter() # initiate AverageMeter for tracking the loss.\n    output_dict = {}\n    preds, trues, ids = [], [], []\n    start = end = time.time() # track the execution time.\n    with tqdm(valid_loader, unit=\"valid_batch\", desc='Validation') as tqdm_valid_loader:\n        for step, batch in enumerate(tqdm_valid_loader):\n            ids_batch = batch.pop(\"id\")\n            original_texts = to_device(collate(batch.pop(\"original_text\")))\n            rewritten_texts = to_device(collate(batch.pop(\"rewritten_text\")))\n            rewrite_prompts = batch.pop(\"rewrite_prompt\")\n            batch_size = len(ids_batch)\n            targets = torch.ones(batch_size, device=device) # -1 for dissimilar, 1 for similar\n            with torch.no_grad():\n                y_preds, y_trues = model(original_texts, rewritten_texts, rewrite_prompts) # forward propagation pass\n                loss = criterion(y_preds, y_trues, targets) # get loss\n            losses.update(loss.item(), batch_size) # update loss function tracking\n            preds.append(y_preds.to('cpu').numpy()) # save predictions\n            trues.append(y_trues.to('cpu').numpy()) # save ground truth\n            ids += ids_batch\n            end = time.time() # get finish time\n\n            # ========== LOG INFO ==========\n            if step % config.PRINT_FREQ == 0 or step == (len(valid_loader)-1):\n                print('EVAL: [{0}/{1}] '\n                      'Elapsed {remain:s} '\n                      'Loss: {loss.avg:.4f} '\n                      .format(step, len(valid_loader),\n                              loss=losses,\n                              remain=timeSince(start, float(step+1)/len(valid_loader))))\n            if config.WANDB:\n                wandb.log({f\"[fold_{fold}] val loss\": losses.val})\n\n    output_dict[\"predictions\"] = np.concatenate(preds)\n    output_dict[\"ground_truths\"] = np.concatenate(trues)\n    output_dict[\"ids\"] = ids\n    return losses.avg, output_dict\n\ndef train_loop(fold=0):\n\n    LOGGER.info(f\"========== Fold: {fold} training ==========\")\n    train = pd.read_parquet(paths.TRAIN_DATA)\n    #train2 = pd.read_csv(paths.TRAIN_DATA2)\n    #train = pd.concat([train, train2], ignore_index=True)\n    valid = pd.read_csv(paths.VALID_DATA)\n    #train = train[train['source']!='dpo'].reset_index(drop=True)\n\n    if config.DEBUG:\n        train = train.head(1000)\n        valid = valid.head(384)\n\n    train['id'] = range(len(train))\n    valid['id'] = range(len(valid))\n    train_embedding = np.load(paths.train_embedding_file)\n    #train_embedding2 = np.load(paths.train_embedding_file2)\n    #train_embedding = np.concatenate([train_embedding, train_embedding2])\n    vaid_embedding = np.load(paths.valid_embedding_file)\n\n    if config.DEBUG:\n        train_embedding = train_embedding[:1000]\n        vaid_embedding = vaid_embedding[:384]\n\n    assert len(train)==len(train_embedding)\n    assert len(valid)==len(vaid_embedding)\n    # ======== DATASETS ==========\n    train_dataset = CustomDataset(config, train, tokenizer, train_embedding)\n    valid_dataset = CustomDataset(config, valid, tokenizer, vaid_embedding)\n\n    # ======== DATALOADERS ==========\n    train_loader = DataLoader(train_dataset,\n                              batch_size=config.BATCH_SIZE_TRAIN, \n                              shuffle=True,\n                              pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset,\n                              batch_size=config.BATCH_SIZE_VALID,\n                              shuffle=False,\n                              pin_memory=True, drop_last=False)\n\n    # ======== MODEL ==========\n    model = CustomModel(config, config_path=None, pretrained=True)\n    torch.save(model.config, paths.OUTPUT_DIR + '/config.pth')\n    model.to(device)\n\n    optimizer_parameters = get_optimizer_params(model,\n                                                encoder_lr=config.LR,\n                                                decoder_lr=config.LR,\n                                                weight_decay=config.WEIGHT_DECAY)\n    optimizer = AdamW(optimizer_parameters,\n                      lr=config.LR,\n                      eps=config.EPS,\n                      betas=config.BETAS)\n\n    scheduler = OneCycleLR(\n        optimizer,\n        max_lr=config.LR,\n        epochs=config.EPOCHS,\n        steps_per_epoch=len(train_loader),\n        pct_start=0.1,\n        anneal_strategy=\"cos\",\n        final_div_factor=100,\n    )\n\n    # ======= LOSS ==========\n    criterion = nn.CosineEmbeddingLoss()\n\n    best_score = -np.inf\n    # ====== ITERATE EPOCHS ========\n    for epoch in range(config.EPOCHS):\n\n        start_time = time.time()\n\n        # ======= TRAIN ==========\n        avg_loss = train_epoch(train_loader, model, criterion, optimizer, epoch, scheduler, device)\n\n        # ======= EVALUATION ==========\n        avg_val_loss, output_dict = valid_epoch(valid_loader, model, criterion, device)\n        predictions = output_dict[\"predictions\"]\n        ground_truths = output_dict[\"ground_truths\"]\n\n        # ======= SCORING ==========\n        score = scs(predictions, ground_truths)\n\n        elapsed = time.time() - start_time\n\n        LOGGER.info(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n        LOGGER.info(f'Epoch {epoch+1} - Score: {score:.4f}')\n\n        if score > best_score:\n            best_score = score\n            LOGGER.info(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n            torch.save(model.state_dict(),\n                        paths.OUTPUT_DIR + f\"/{config.CKPT.replace('/', '_')}_fold_{fold}_best.pth\")\n            best_model_predictions = predictions\n\n    valid.loc[:, \"preds\"] = best_model_predictions.tolist()\n    valid.loc[:, \"trues\"] = ground_truths.tolist()\n\n    torch.cuda.empty_cache()\n    gc.collect()\n\n    return valid\n\nif __name__ == '__main__':\n    def get_result(oof_df):\n        trues = oof_df[\"trues\"].values\n        preds = oof_df[\"preds\"].values\n        score = scs(preds, trues)\n        LOGGER.info(f'Score: {score:<.4f}')\n\n\n    oof_df = train_loop()\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### seq2seq的检索库生成","metadata":{}},{"cell_type":"code","source":"%%writefile prompts_embedding_index_generate.py\nimport sys\nfrom sentence_transformers import SentenceTransformer, models\nimport pandas as pd\nimport gc\nimport numpy as np\n\ndf = pd.read_csv(f\"prompts_df.csv\",)\n\n#df = df.rename(columns={'text':\"rewrite_prompt\"})\ncontexts = list(df['rewrite_prompt'])\nimport faiss\n\nimport pandas as pd\nimport time\nfrom tqdm import tqdm\nimport numpy as np\nfrom sentence_transformers import SentenceTransformer\nimport pickle\nmodel =  SentenceTransformer('sentence-transformers/sentence-t5-base')\nmodel.max_seq_length = 512\n\nencoded_data = model.encode(contexts, batch_size=64, device='cuda', show_progress_bar=True, convert_to_tensor=True, normalize_embeddings=True)\nencoded_data = encoded_data.detach().cpu().numpy()\nencoded_data = np.asarray(encoded_data.astype('float32'))\n#np.save('prompts_embedding.npy',encoded_data)\ndf['rewrite_prompt'].to_csv('prompts_df.csv', index=False)\n\nindex = faiss.IndexFlatIP(768)\nindex.add(encoded_data)\nfaiss.write_index(index, 'prompts_embedding.index')","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### seq2seq推理","metadata":{}},{"cell_type":"code","source":"%%writefile infer_seq2seq.py\nimport gc\nimport math\nimport matplotlib.pyplot as plt\nimport multiprocessing\nimport numpy as np\nimport os\nimport pandas as pd\nimport random\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nfrom glob import glob\nfrom torch.nn import Parameter\nfrom torch.utils.data import DataLoader, Dataset\nfrom tqdm import tqdm\nfrom typing import Dict, List\n\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nprint(f\"Current device is: {device}\")\n\nimport tokenizers\nimport transformers\nfrom transformers import AutoTokenizer, AutoModel, AutoConfig\nfrom transformers import get_linear_schedule_with_warmup, get_cosine_schedule_with_warmup\n\n\nclass config:\n    BATCH_SIZE_TEST = 4\n    DEBUG = False\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    GRADIENT_CHECKPOINTING = True\n    MODEL = \"microsoft/deberta-v3-base\"\n    MAX_LEN = 512\n    NUM_WORKERS = 0 # multiprocessing.cpu_count()\n    SEED = 20\n\nclass paths:\n    TEST_CSV = \"/kaggle/input/llm-prompt-recovery/test.csv\"\n    TOKENIZER = '/kaggle/input/llmpr-models3/exp14/tokenizer'\n    \nmodel_weights = [\n                \n                '/kaggle/input/llmpr-models3/exp14/deberta-v3-large_fold_0_best.pth',\n                ]\nmodel_configs = [\n                 '/kaggle/input/llmpr-models3/exp14/config.pth',\n                 \n               ]\nmodel_weights\n\ndef get_config_dict(config):\n    \"\"\"\n    Return the config, which is originally a class, as a Python dictionary.\n    \"\"\"\n    config_dict = dict((key, value) for key, value in config.__dict__.items() \n    if not callable(value) and not key.startswith('__'))\n    return config_dict\n\n\ndef seed_everything(seed=20):\n    \"\"\"Seed everything to ensure reproducibility\"\"\"\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    \n    \ndef to_device(inputs, device: str = device):\n    return {k: v.to(device) for k, v in inputs.items()}\n\nseed_everything(seed=config.SEED)\n\ntest_df = pd.read_csv(paths.TEST_CSV)\nprint(f\"Dataframe has shape: {test_df.shape}\")\ntest_df.head()\n\ntokenizer = AutoTokenizer.from_pretrained(paths.TOKENIZER)\nprint(tokenizer)\n\ntest_df['original_text'] = test_df['original_text'].fillna(\"\")\ntest_df['rewritten_text'] = test_df['rewritten_text'].fillna(\"\")\n\ndef prepare_input(cfg: type, text: np.ndarray, tokenizer):\n    \"\"\"\n    This function tokenizes the input text with the configured padding and truncation. Then,\n    returns the input dictionary, which contains the following keys: \"input_ids\",\n    \"token_type_ids\" and \"attention_mask\". Each value is a torch.tensor.\n    :param cfg: configuration class.\n    :param text: a numpy array where each value is a text as string.\n    :return inputs: python dictionary where values are torch tensors.\n    \"\"\"\n    inputs = tokenizer.encode_plus(\n        text, \n        return_tensors=None, \n        add_special_tokens=True, \n        max_length=cfg.MAX_LEN,\n        padding='max_length', # TODO: check padding to max sequence in batch\n        truncation=True\n    )\n    for k, v in inputs.items():\n        inputs[k] = torch.tensor(v, dtype=torch.long) # TODO: check dtypes\n    return inputs\n\n\ndef collate(inputs):\n    \"\"\"\n    It truncates the inputs to the maximum sequence length in the batch. \n    \"\"\"\n    mask_len = int(inputs[\"attention_mask\"].sum(axis=1).max()) # Get batch's max sequence length\n    for k, v in inputs.items():\n        inputs[k] = inputs[k][:,:mask_len]\n    return inputs\n\n\nclass CustomDataset(Dataset):\n    def __init__(self, cfg, df, tokenizer):\n        self.cfg = cfg\n        self.original_texts = df['original_text'].values\n        self.rewritten_texts = df['rewritten_text'].values\n        self.rewrite_prompts = []\n        self.text_ids = df['id'].astype(str).values\n        self.tokenizer = tokenizer\n        \n    def __len__(self):\n        return len(self.text_ids)\n\n    def __getitem__(self, item):\n        output = {}\n        output[\"original_text\"] = prepare_input(self.cfg, self.original_texts[item], self.tokenizer)\n        output[\"rewritten_text\"] = prepare_input(self.cfg, self.rewritten_texts[item], self.tokenizer)\n        output[\"rewrite_prompt\"] = []\n        output[\"id\"] = self.text_ids[item]\n        return output\n\nclass MeanPooling(nn.Module):\n    def __init__(self):\n        super(MeanPooling, self).__init__()\n        \n    def forward(self, last_hidden_state, attention_mask):\n        input_mask_expanded = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()\n        sum_embeddings = torch.sum(last_hidden_state * input_mask_expanded, 1)\n        sum_mask = input_mask_expanded.sum(1)\n        sum_mask = torch.clamp(sum_mask, min=1e-9)\n        mean_embeddings = sum_embeddings / sum_mask\n        return mean_embeddings\n    \n\nclass CustomModel(nn.Module):\n    def __init__(self, cfg, config_path=None, mode: str = \"test\", pretrained=False):\n        super().__init__()\n        self.cfg = cfg\n        self.mode = mode\n        self.dropout = 0.2\n        # Load config by inferencing it from the model name.\n        if config_path is None: \n            self.config = AutoConfig.from_pretrained(cfg.MODEL, output_hidden_states=True)\n            self.config.hidden_dropout = 0.\n            self.config.hidden_dropout_prob = 0.\n            self.config.attention_dropout = 0.\n            self.config.attention_probs_dropout_prob = 0.\n        # Load config from a file.\n        else:\n            self.config = torch.load(config_path)\n        \n        if pretrained:\n            self.model = AutoModel.from_pretrained(cfg.MODEL, config=self.config)\n        else:\n            self.model = AutoModel.from_config(self.config)\n        \n        if self.cfg.GRADIENT_CHECKPOINTING:\n            self.model.gradient_checkpointing_enable()\n          \n        self.t5_encoder = None #hub.KerasLayer(cfg.T5_MODEL)\n        self.pool = MeanPooling()\n        self.head = nn.Sequential(\n            nn.Linear(self.config.hidden_size*4, 32768),\n            nn.BatchNorm1d(32768),\n            nn.ReLU(),\n            nn.Linear(32768, 768),\n        )\n        self._init_weights(self.head)\n        \n    def _init_weights(self, module):\n        \"\"\"\n        This method initializes weights for different types of layers. The type of layers \n        supported are nn.Linear, nn.Embedding and nn.LayerNorm.\n        \"\"\"\n        if isinstance(module, nn.Linear):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.bias is not None:\n                module.bias.data.zero_()\n        elif isinstance(module, nn.Embedding):\n            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n            if module.padding_idx is not None:\n                module.weight.data[module.padding_idx].zero_()\n        elif isinstance(module, nn.LayerNorm):\n            module.bias.data.zero_()\n            module.weight.data.fill_(1.0)\n        \n    def feature(self, inputs):\n        outputs = self.model(**inputs)\n        #last_hidden_states = outputs[1]\n        feature1 = self.pool(outputs.hidden_states[-1], inputs['attention_mask'])\n        feature2 = self.pool(outputs.hidden_states[-2], inputs['attention_mask'])\n        return torch.cat([feature1, feature2], dim=1)\n\n    def forward(self, original_texts, rewritten_texts, rewrite_prompts):\n        \"\"\"\n        This method makes a forward pass through the model, the MeanPooling layer and finally\n        then through the Linear layer to get a regression value.\n        \"\"\"\n        original_texts_feature = self.feature(original_texts) # shape (batch_size, 768)\n        rewritten_texts_feature = self.feature(rewritten_texts) # shape (batch_size, 768)\n        feature = torch.cat([original_texts_feature, rewritten_texts_feature], dim=1) # shape (batch_size, 768 * 2)\n        output = self.head(feature)\n        \n        if self.mode == \"train\":\n            prompt_embedding = torch.tensor(self.t5_encoder(rewrite_prompts)[0].numpy(), device=self.cfg.DEVICE) # shape (batch_size, 768)\n        else:\n            prompt_embedding = []\n            \n        return output, prompt_embedding\n    \n\ndef sharpened_cosine_similarity(k: np.ndarray, s: np.ndarray, p: int = 3, q: float = 1e-10):\n    \"\"\"\n    Computes Sharpened Cosine Similarity (SCS) between two numpy arrays of shape (N,).\n    :param k: prediction embedding.\n    :param s: ground truth embedding.\n    :param p: SCS power.\n    :param q: small value for numerical stability.\n    :return score: SCS score.\n    \"\"\"\n    dot_product = np.dot(k, s)\n    norm_k = np.linalg.norm(k)\n    norm_s = np.linalg.norm(s)\n    # Compute the cosine similarity with added term q for numerical stability\n    cosine_similarity = dot_product / (norm_k * (norm_s + q))\n    # Compute the sharpened cosine similarity\n    score = np.sign(dot_product) * (cosine_similarity ** p)\n    return score\n\n\ndef scs(k_batch: np.ndarray, s_batch: np.ndarray, p: int = 3, q: float = 1e-10):\n    \"\"\"\n    Computes Sharpened Cosine Similarity (SCS) between two batches of numpy arrays of shape (batch_size, N).\n    :param k: prediction embedding.\n    :param s: ground truth embedding.\n    :param p: SCS power.\n    :param q: small value for numerical stability.\n    :return score: mean SCS score for the batch.\n    \"\"\"\n    bs = k_batch.shape[0]\n    scores = []\n    for item in range(bs):\n        k = k_batch[item]\n        s = s_batch[item]\n        scores.append(sharpened_cosine_similarity(k, s, p, q))\n    score = np.mean(scores)\n    return score\n\ndef inference_fn(model_weight, config, test_df, tokenizer, device, model_config):\n    # ======== DATASETS ==========\n    test_dataset = CustomDataset(config, test_df, tokenizer)\n    \n    # ======== DATALOADERS ==========\n    test_loader = DataLoader(\n        test_dataset,\n        batch_size=config.BATCH_SIZE_TEST,\n        shuffle=False,\n        num_workers=0,\n        pin_memory=True, drop_last=False\n    )\n    \n    # ======== MODEL ==========\n    model = CustomModel(config, config_path=model_config, pretrained=False)\n    state = torch.load(model_weight)\n    model.load_state_dict(state)\n    model.to(device)\n    model.eval() # set model in evaluation mode\n    output_dict = {}\n    preds, ids = [], []\n    with tqdm(test_loader, unit=\"test_batch\", desc='Test') as tqdm_test_loader:\n        for step, batch in enumerate(tqdm_test_loader):\n            ids_batch = batch.pop(\"id\")\n            original_texts = to_device(collate(batch.pop(\"original_text\")))\n            rewritten_texts = to_device(collate(batch.pop(\"rewritten_text\")))\n            rewrite_prompts = []\n            batch_size = len(ids_batch)\n            targets = torch.ones(batch_size, device=device) # -1 for dissimilar, 1 for similar\n            with torch.no_grad():\n                y_preds, _ = model(original_texts, rewritten_texts, rewrite_prompts)            \n            preds.append(y_preds.to('cpu').numpy()) # save predictions\n            ids += ids_batch          \n    output_dict[\"predictions\"] = np.concatenate(preds) \n    output_dict[\"ids\"] = ids\n    return output_dict\n\npreds = []\n\nfor model_weight, model_config in zip(model_weights, model_configs):\n    predictions = inference_fn(model_weight, config, test_df, tokenizer, device, model_config)\n    predictions = predictions[\"predictions\"]\n    predictions = torch.nn.functional.normalize(torch.from_numpy(predictions), p=2, dim=1).numpy()\n    preds.append(predictions)\n    \npreds = np.mean(preds, axis=0)\n\nimport faiss\nfrom faiss import write_index, read_index, read_VectorTransform\n\nprompts_embedding_index = read_index(\"./prompts_embedding.index\")\nsearch_score, search_index = prompts_embedding_index.search(preds, 1)\nprompts_df = pd.read_csv(\"./prompts_df.csv\")\nprompts_df.head()\n\npred_prompts = []\n\nfor i, (scr, idx) in tqdm(enumerate(zip(search_score, search_index)), total=len(search_score)):\n    scr_idx = idx\n    p = prompts_df.loc[scr_idx, \"rewrite_prompt\"].tolist()\n    pred_prompts.append(''.join(p))\n\nvalues = pred_prompts\n\nsubmission = pd.DataFrame()\nsubmission[\"id\"] = test_df[\"id\"]\nsubmission[\"rewrite_prompt\"] = values\nsubmission.to_csv(\"pred1.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T01:37:03.981952Z","iopub.execute_input":"2024-04-10T01:37:03.982275Z","iopub.status.idle":"2024-04-10T01:37:03.997719Z","shell.execute_reply.started":"2024-04-10T01:37:03.982245Z","shell.execute_reply":"2024-04-10T01:37:03.996868Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python infer_seq2seq.py","metadata":{"execution":{"iopub.status.busy":"2024-04-10T01:37:04.000096Z","iopub.execute_input":"2024-04-10T01:37:04.000355Z","iopub.status.idle":"2024-04-10T01:39:40.146359Z","shell.execute_reply.started":"2024-04-10T01:37:04.000333Z","shell.execute_reply":"2024-04-10T01:39:40.145263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 开源微调模型sft-phi2的推理","metadata":{}},{"cell_type":"code","source":"%%writefile infer_phi.py\nimport numpy as np\nimport pandas as pd\nfrom tqdm.auto import tqdm\n\nimport torch\n\nfrom peft import PeftConfig, PeftModel\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\ninput_token_len = 1024\noutput_token_len = 100\ntest_df = pd.read_csv('/kaggle/input/llm-prompt-recovery/test.csv')\nbase_model_name = \"/kaggle/input/phi/transformers/2/1\"#/kaggle/input/phi/transformers/2/1\nadapter_model_name = \"/kaggle/input/phi2-public-data-sft-adapter/pytorch/public-data-sft/1/phi2_public_data_sft\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntokenizer = AutoTokenizer.from_pretrained(base_model_name,trust_remote_code=True)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\n    \nmodel = AutoModelForCausalLM.from_pretrained(base_model_name,trust_remote_code=True)\nmodel = PeftModel.from_pretrained(model, adapter_model_name)\nmodel.to(device)\nmodel.eval()\nprint('model loaded !!')\ndef text_generate(ori_text, rew_text,model, tokenizer, stop_tokens=['.',';',':','<|endoftext|>'], input_max_len=512, output_len=20, device='cuda'):\n    prompt = f\"Instruct: Original Text:{ori_text}\\nRewritten Text:{rew_text}\\nWrite a prompt that was likely given to the LLM to rewrite original text to rewritten text.\\nOutput:\"\n    inputs = tokenizer(prompt, max_length=input_max_len, truncation=True, return_tensors=\"pt\", return_attention_mask=False)\n    output_start_index = len(inputs.input_ids[0])\n    inputs = {k:v.to(device) for k,v in inputs.items()}\n    outputs = model.generate(**inputs,\n                             do_sample=False,\n                             max_new_tokens=output_len,\n                             pad_token_id=tokenizer.pad_token_id,\n                             eos_token_id=tokenizer.convert_tokens_to_ids(stop_tokens),\n                            )\n    text = tokenizer.batch_decode(outputs,skip_special_tokens=True,clean_up_tokenization_spaces=False)[0]\n    start_index = text.find('Output:')\n    generated_text = text[start_index+len('Output:'):].strip()[:-1]\n    return generated_text\n\nimport nltk\nfrom nltk import sent_tokenize\nimport re\nrewrite_prompts = []\nfor i, row in tqdm(test_df.iterrows(), total=len(test_df)):\n    prompt = 'Please improve this text.'\n    try:\n        prompt = text_generate(row['original_text'],\n                               row['rewritten_text'],\n                               model,\n                               tokenizer,\n                               ['.',';',':','<|endoftext|>'],\n                               input_token_len,\n                               output_token_len,\n                               device,\n                              )\n    except:\n        pass\n        \n    rewrite_prompts.append(prompt)\n\n\n\ntest_df['rewrite_prompt'] = rewrite_prompts\nsub_df = test_df[['id', 'rewrite_prompt']]\nsub_df.to_csv('pred2.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T01:39:40.147792Z","iopub.execute_input":"2024-04-10T01:39:40.148063Z","iopub.status.idle":"2024-04-10T01:39:40.156353Z","shell.execute_reply.started":"2024-04-10T01:39:40.148037Z","shell.execute_reply":"2024-04-10T01:39:40.155469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python infer_phi.py","metadata":{"execution":{"iopub.status.busy":"2024-04-10T01:39:40.157569Z","iopub.execute_input":"2024-04-10T01:39:40.157891Z","iopub.status.idle":"2024-04-10T01:40:51.504364Z","shell.execute_reply.started":"2024-04-10T01:39:40.157862Z","shell.execute_reply":"2024-04-10T01:40:51.50347Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### LLM模型的zero-shot推理","metadata":{}},{"cell_type":"code","source":"%%writefile mistral_infer.py\nimport torch\nimport random\nimport numpy as np\nimport pandas as pd\nimport gc\nimport time\n\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n\n#https://github.com/Lightning-AI/lit-gpt/issues/327\ntorch.backends.cuda.enable_mem_efficient_sdp(False)\ntorch.backends.cuda.enable_flash_sdp(False)\n\nif (not torch.cuda.is_available()): print(\"Sorry - GPU required!\")\n    \nimport logging\nlogging.getLogger('transformers').setLevel(logging.ERROR)\n#this can help speed up inference\nmax_new_tokens = 30\n\n#output test is trimmed according to this\nmax_sentences_in_response = 1\nmodel_name = '/kaggle/input/mistral-7b-it-v02'\ntokenizer = AutoTokenizer.from_pretrained(model_name) \n\n# Load base model(Mistral 7B)\nbnb_config = BitsAndBytesConfig(  \n    load_in_4bit= True,\n    bnb_4bit_quant_type= \"nf4\",\n    bnb_4bit_compute_dtype= torch.bfloat16,\n    bnb_4bit_use_double_quant= False,\n)\n\nmodel = AutoModelForCausalLM.from_pretrained(\n        model_name,\n        quantization_config=bnb_config,\n        torch_dtype=torch.bfloat16,\n        device_map=\"auto\",\n        trust_remote_code=True,\n)\n#original text prefix\norig_prefix = \"Original Text:\"\n\n#mistral \"response\"\nllm_response_for_rewrite = \"Provide the new text and I will tell you what new element was added or change in tone was made to improve it - with no references to the original.  I will avoid mentioning names of characters.  It is crucial no person, place or thing from the original text be mentioned.  For example - I will not say things like 'change the puppet show into a book report' - I would just say 'Please improve this text using the writing style of a book report'.  If the original text mentions a specific idea, person, place, or thing - I will not mention it in my answer.  For example if there is a 'dog' or 'office' in the original text - the word 'dog' or 'office' must not be in my response.  My answer will be a single sentence.\"\n\n#modified text prefix\nrewrite_prefix = \"Re-written Text:\"\n\n#provided as start of Mistral response (anything after this is used as the prompt)\n#providing this as the start of the response helps keep things relevant\nresponse_start = \"The request was: \"\n\n#added after response_start to prime mistral\n#\"Improve this\" or \"Improve this text\" resulted in non-answers.  \n#\"Improve this text by\" seems to product good results\nresponse_prefix = \"Please improve this text using the writing style\"\n\n#well-scoring baseline text\n#thanks to: https://www.kaggle.com/code/rdxsun/lb-0-61\nbase_line = 'Please improve this text using the writing style with maintaining the original meaning but altering the tone.' \n\n#these will all be given to Mistral before each and every prompt\n#original_text\n#rewritten_text\n#prompt\n\nexamples_sequences = [\n    (\n        \"Hey there! Just a heads up: our friendly dog may bark a bit, but don't worry, he's all bark and no bite!\",\n        \"Warning: Protective dog on premises. May exhibit aggressive behavior. Ensure personal safety by maintaining distance and avoiding direct contact.\",\n        \"Please improve this text using the writing style of a warning.\"\n    ),\n\n    (\n        \"A lunar eclipse happens when Earth casts its shadow on the moon during a full moon. The moon appears reddish because Earth's atmosphere scatters sunlight, some of which refracts onto the moon's surface. Total eclipses see the moon entirely in Earth's shadow; partial ones occur when only part of the moon is shadowed.\",\n        \"Yo check it, when the Earth steps in, takes its place, casting shadows on the moon's face. It's a full moon night, the scene's set right, for a lunar eclipse, a celestial sight. The moon turns red, ain't no dread, it's just Earth's atmosphere playing with sunlight's thread, scattering colors, bending light, onto the moon's surface, making the night bright. Total eclipse, the moon's fully in the dark, covered by Earth's shadow, making its mark. But when it's partial, not all is shadowed, just a piece of the moon, slightly furrowed. So that's the rap, the lunar eclipse track, a dance of shadows, with no slack. Earth, moon, and sun, in a cosmic play, creating the spectacle we see today.\",\n        \"Please improve this text using the writing style of a rap.\"\n    ),\n    \n    (\n        \"Drinking enough water each day is crucial for many functions in the body, such as regulating temperature, keeping joints lubricated, preventing infections, delivering nutrients to cells, and keeping organs functioning properly. Being well-hydrated also improves sleep quality, cognition, and mood.\",\n        \"Arrr, crew! Sail the health seas with water, the ultimate treasure! It steadies yer body's ship, fights off plagues, and keeps yer mind sharp. Hydrate or walk the plank into the abyss of ill health. Let's hoist our bottles high and drink to the horizon of well-being!\",\n        \"Please improve this text using the writing style of a sea pirate.\"\n    ),\n    \n    (\n        \"In a bustling cityscape, under the glow of neon signs, Anna found herself at the crossroads of endless possibilities. The night was young, and the streets hummed with the energy of life. Drawn by the allure of the unknown, she wandered through the maze of alleys and boulevards, each turn revealing a new facet of the city's soul. It was here, amidst the symphony of urban existence, that Anna discovered the magic hidden in plain sight, the stories and dreams that thrived in the shadows of skyscrapers.\",\n        \"On an ordinary evening, amidst the cacophony of a neon-lit city, Anna stumbled upon an anomaly - a door that defied the laws of time and space. With the curiosity of a cat, she stepped through, leaving the familiar behind. Suddenly, she was adrift in the stream of time, witnessing the city's transformation from past to future, its buildings rising and falling like the breaths of a sleeping giant.\",\n        \"Please improve this text using the writing style with time travel topic.\"\n    ),\n    \n    (\n        \"Late one night in the research lab, Dr. Evelyn Archer was on the brink of a breakthrough in artificial intelligence. Her fingers danced across the keyboard, inputting the final commands into the system. The lab was silent except for the hum of machinery and the occasional beep of computers. It was in this quiet orchestra of technology that Evelyn felt most at home, on the cusp of unveiling a creation that could change the world.\",\n        \"In the deep silence of the lab, under the watchful gaze of the moon, Dr. Evelyn Archer found herself not alone. Beside her, the iconic red eye of HAL 9000 flickered to life, a silent partner in her nocturnal endeavor. 'Good evening, Dr. Archer,' HAL's voice filled the room, devoid of warmth yet comforting in its familiarity. Together, they were about to initiate a test that would intertwine the destiny of human and artificial intelligence forever. As Evelyn entered the final command, HAL processed the data with unparalleled precision, a testament to the dawn of a new era.\",\n        \"Please improve this text using the writing style with an intelligent computer.\"\n    ),\n    \n    (\n        \"The park was empty, save for a solitary figure sitting on a bench, lost in thought. The quiet of the evening was punctuated only by the occasional rustle of leaves, offering a moment of peace in the chaos of city life.\",\n        \"Beneath the cloak of twilight, the park transformed into a realm of solitude and reflection. There, seated upon an ancient bench, was a lone soul, a guardian of secrets, enveloped in the serenity of nature's whispers. The dance of the leaves in the gentle breeze sang a lullaby to the tumult of the urban heart.\",\n        \"Please improve this text using the writing style to be more poetic.\"\n    ),\n    \n    (\n        \"The annual town fair was bustling with activity, from the merry-go-round spinning with laughter to the game booths challenging eager participants. Amidst the excitement, a figure in a cloak moved silently, almost invisibly, among the crowd, observing everything with keen interest but participating in none.\",\n        \"Beneath the riot of color and sound that marked the town's annual fair, a solitary figure roamed, known to the few as Eldrin the Enigmatic. Clad in a cloak that shimmered with the whispers of the arcane, Eldrin moved with the grace of a shadow, his gaze piercing the veneer of festivity to the magic beneath. As a master of the mystic arts, he sought not the laughter of the crowds but the silent stories woven into the fabric of the fair. With a flick of his wrist, he could coax wonder from the mundane, transforming the ordinary into spectacles of shimmering illusion, his true participation hidden within the folds of mystery.\",\n        \"Please improve this text using the writing style by adding a magician.\"\n    ),\n    \n    (\n        \"The startup team sat in the dimly lit room, surrounded by whiteboards filled with ideas, charts, and plans. They were on the brink of launching a new app designed to make home maintenance effortless for homeowners. The app would connect users with local service providers, using a sophisticated algorithm to match needs with skills and availability. As they debated the features and marketing strategies, the room felt charged with the energy of creation and the anticipation of what was to come.\",\n        \"In the quiet before dawn, a small group of innovators gathered, their mission: to simplify home maintenance through technology. But their true journey began with the unexpected addition of Max, a talking car with a knack for solving problems. 'Let me guide you through this maze of decisions,' Max offered, his dashboard flickering to life.\",\n        \"Please improve this text using the writing style by adding a talking car.\"\n    ),\n    \n        \n\n    \n    \n]\n\ndef remove_numbered_list(text):\n    final_text_paragraphs = [] \n    for line in text.split('\\n'):\n        # Split each line at the first occurrence of '. '\n        parts = line.split('. ', 1)\n        # If the line looks like a numbered list item, remove the numbering\n        if len(parts) > 1 and parts[0].isdigit():\n            final_text_paragraphs.append(parts[1])\n        else:\n            # If it doesn't look like a numbered list item, include the line as is\n            final_text_paragraphs.append(line)\n\n    return '  '.join(final_text_paragraphs)\n\n\n#trims LLM output to just the response\ndef trim_to_response(text):\n    terminate_string = \"[/INST]\"\n    text = text.replace('</s>', '')\n    #just in case it puts things in quotes\n    text = text.replace('\"', '')\n    text = text.replace(\"'\", '')\n\n    last_pos = text.rfind(terminate_string)\n    return text[last_pos + len(terminate_string):] if last_pos != -1 else text\n\n#looks for response_start / returns only text that occurs after\ndef extract_text_after_response_start(full_text):\n    parts = full_text.rsplit(response_start, 1)  # Split from the right, ensuring only the last occurrence is considered\n    if len(parts) > 1:\n        return parts[1].strip()  # Return text after the last occurrence of response_start\n    else:\n        return full_text  # Return the original text if response_start is not found\n\n    \n#trims text to requested number of sentences (or first LF or double-space sequence)\ndef trim_to_first_x_sentences_or_lf(text, x):\n    if x <= 0:\n        return \"\"\n\n    # Any double-spaces dealt with as linefeed\n    text = text.replace(\"  \", \"\\n\")\n\n    # Split text at the first linefeed\n    text_chunks = text.split('\\n', 1)\n    first_chunk = text_chunks[0]\n\n    # Split the first chunk into sentences, considering the space after each period\n    sentences = [sentence.strip() for sentence in first_chunk.split('.') if sentence]\n\n    # If there's a linefeed, return the text up to the first linefeed\n    if len(text_chunks) > 1:\n        # Check if the first chunk has fewer sentences than x, and if so, just return it\n        if len(sentences) < x:\n            trimmed_text = first_chunk\n        else:\n            # Otherwise, trim to x sentences within the first chunk\n            trimmed_text = '. '.join(sentences[:x]).strip()\n    else:\n        # If there's no linefeed, determine if the number of sentences is less than or equal to x\n        if len(sentences) <= x:\n            trimmed_text = '. '.join(sentences).strip()  # Ensure space is preserved after periods\n        else:\n            # Otherwise, return the first x sentences, again ensuring space after periods\n            trimmed_text = '. '.join(sentences[:x]).strip()\n\n    # Add back the final period if it was removed and the text needs to end with a sentence.\n    if len(sentences) > 0 and not trimmed_text.endswith('.'):\n        trimmed_text += '.'\n\n    return trimmed_text\n\ndef get_prompt(orig_text, transformed_text):\n    stop_tokens = ['.',':']\n    messages = []\n\n    # Append example sequences\n    for example_text, example_rewrite, example_prompt in examples_sequences:\n        messages.append({\"role\": \"user\", \"content\": f\"{orig_prefix} {example_text}\"})\n        messages.append({\"role\": \"assistant\", \"content\": llm_response_for_rewrite})\n        messages.append({\"role\": \"user\", \"content\": f\"{rewrite_prefix} {example_rewrite}\"})\n        messages.append({\"role\": \"assistant\", \"content\": f\"{response_start} {example_prompt}\"})\n\n    #actual prompt\n    messages.append({\"role\": \"user\", \"content\": f\"{orig_prefix} {orig_text}\"})\n    messages.append({\"role\": \"assistant\", \"content\": llm_response_for_rewrite})\n    messages.append({\"role\": \"user\", \"content\": f\"{rewrite_prefix} {transformed_text}\"})\n    messages.append({\"role\": \"assistant\", \"content\": f\"{response_start}\"})\n        \n    #give it to Mistral\n    decode_ids = tokenizer.encode(response_prefix, add_special_tokens=False)\n    model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\")\n    \n    output_start_index = len(model_inputs[0])\n    force_decoder_ids = []\n    for i, did in enumerate(decode_ids):\n        force_decoder_ids.append([i+output_start_index, did])\n    \n    model_inputs = model_inputs.to(\"cuda\") \n    generated_ids = model.generate(model_inputs, max_new_tokens=max_new_tokens, \n                                   pad_token_id=tokenizer.eos_token_id,\n                                   eos_token_id=tokenizer.convert_tokens_to_ids(stop_tokens),\n                                   forced_decoder_ids = force_decoder_ids,\n                                  )\n\n    #decode and trim to actual response\n    decoded = tokenizer.batch_decode(generated_ids)\n    just_response = trim_to_response(decoded[0])        \n    final_text = extract_text_after_response_start(just_response)\n        \n    #mistral has been replying with numbered lists - clean them up....\n    final_text = remove_numbered_list(final_text)\n        \n    #mistral v02 tends to respond with the input after providing the answer - this tries to trim that down\n    final_text = trim_to_first_x_sentences_or_lf(final_text, max_sentences_in_response)\n    \n    #default to baseline if empty or unusually short\n    if len(final_text) < 15:\n        final_text = base_line\n        return final_text\n    final_text = final_text[:-1] + ', maintaining the original meaning but altering the tone.'\n    return final_text\n\ntest_df = pd.read_csv(\"/kaggle/input/llm-prompt-recovery/test.csv\")\n\nfor index, row in test_df.iterrows():\n    result = get_prompt(row['original_text'], row['rewritten_text'])\n    print(result)\n    test_df.at[index, 'rewrite_prompt'] = result\n    \ntest_df = test_df[['id', 'rewrite_prompt']]\ntest_df.to_csv('pred3.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T01:41:29.841805Z","iopub.execute_input":"2024-04-10T01:41:29.842154Z","iopub.status.idle":"2024-04-10T01:41:29.858836Z","shell.execute_reply.started":"2024-04-10T01:41:29.842123Z","shell.execute_reply":"2024-04-10T01:41:29.857866Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python mistral_infer.py","metadata":{"execution":{"iopub.status.busy":"2024-04-10T01:41:29.859955Z","iopub.execute_input":"2024-04-10T01:41:29.860212Z","iopub.status.idle":"2024-04-10T01:43:50.355008Z","shell.execute_reply.started":"2024-04-10T01:41:29.860191Z","shell.execute_reply":"2024-04-10T01:43:50.354068Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 集成三种模型的结果","metadata":{}},{"cell_type":"code","source":"import pandas as pd\np1 = pd.read_csv('pred1.csv').sort_values(['id']).reset_index(drop=True).fillna('')\np2 = pd.read_csv('pred2.csv').sort_values(['id']).reset_index(drop=True).fillna('')\np3 = pd.read_csv('pred3.csv').sort_values(['id']).reset_index(drop=True).fillna('')\np1['rewrite_prompt'] = p1['rewrite_prompt'].map(str)+' '+p2['rewrite_prompt'].map(str)+' '+p3['rewrite_prompt'].map(str)\nprint(p1['rewrite_prompt'].iloc[0])\np1.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-10T01:43:50.356453Z","iopub.execute_input":"2024-04-10T01:43:50.356731Z","iopub.status.idle":"2024-04-10T01:43:50.783945Z","shell.execute_reply.started":"2024-04-10T01:43:50.356704Z","shell.execute_reply":"2024-04-10T01:43:50.78299Z"},"trusted":true},"execution_count":null,"outputs":[]}]}