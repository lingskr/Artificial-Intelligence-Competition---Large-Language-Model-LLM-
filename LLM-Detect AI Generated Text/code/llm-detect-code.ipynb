{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":61542,"databundleVersionId":7516023,"sourceType":"competition"},{"sourceId":4593763,"sourceType":"datasetVersion","datasetId":2677109},{"sourceId":6867914,"sourceType":"datasetVersion","datasetId":3946973},{"sourceId":6890527,"sourceType":"datasetVersion","datasetId":3942644},{"sourceId":6901341,"sourceType":"datasetVersion","datasetId":3960967},{"sourceId":6977472,"sourceType":"datasetVersion","datasetId":4005256},{"sourceId":7060310,"sourceType":"datasetVersion","datasetId":3944051},{"sourceId":7082713,"sourceType":"datasetVersion","datasetId":4039374},{"sourceId":7264407,"sourceType":"datasetVersion","datasetId":3954249,"isSourceIdPinned":true},{"sourceId":7294503,"sourceType":"datasetVersion","datasetId":4210720},{"sourceId":7446561,"sourceType":"datasetVersion","datasetId":4334378},{"sourceId":150646249,"sourceType":"kernelVersion"},{"sourceId":153376184,"sourceType":"kernelVersion"},{"sourceId":157540400,"sourceType":"kernelVersion"},{"sourceId":157614545,"sourceType":"kernelVersion"},{"sourceId":157615320,"sourceType":"kernelVersion"},{"sourceId":157615776,"sourceType":"kernelVersion"},{"sourceId":157883106,"sourceType":"kernelVersion"},{"sourceId":157935300,"sourceType":"kernelVersion"},{"sourceId":158013112,"sourceType":"kernelVersion"},{"sourceId":158032599,"sourceType":"kernelVersion"},{"sourceId":158296942,"sourceType":"kernelVersion"},{"sourceId":158591974,"sourceType":"kernelVersion"}],"dockerImageVersionId":30558,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#解决方案包括三种建模：1、基于TFIDF统计特征的线性模型预测；\n                #2、基于与比赛数据相似的特定文本数据的distilRoBERTa的文本二分类模型预测；\n                #3、基于大规模AI生成数据和来自于网络开源数据集（人为写作）构成的多样性数据文本，使用deberta-v3-small训练二分类模型预测。","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 基于TFIDF的线性模型：\n* 使用测试集文本预训练分词器，将所得分词器tokenizer用来分词（token）所有文本（包括训练集与测试集）\n* 分词完成后使用TFIDF获取Ngram（3,5）文本统计特征\n* 将上述特征输入MultinomialNB与SGDClassifier构成的融合分类器训练，然后预测结果\n* 伪标签操作：将测试集部分预测结果置信度较高部分加入训练集一起训练，重复上述操作得到线性分类器最终结果\n\n#### 数据集\n* https://www.kaggle.com/datasets/thedrcat/daigt-v2-train-dataset\n* https://www.kaggle.com/datasets/alejopaullier/argugpt\n</br>将上述数据移除相似度大于0.9的样本后得到","metadata":{}},{"cell_type":"code","source":"%%writefile lin_infer.py\nimport sys\nimport gc\nfrom scipy.sparse import vstack\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\nimport glob\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.stats import rankdata\nfrom tokenizers import (\n    decoders,\n    models,\n    normalizers,\n    pre_tokenizers,\n    processors,\n    trainers,\n    Tokenizer,\n)\n\nfrom datasets import Dataset\nfrom tqdm.auto import tqdm\nfrom transformers import PreTrainedTokenizerFast\nfrom tokenizers.normalizers import (Sequence, Lowercase, NFD, \n                                   StripAccents)\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import VotingClassifier\n#论坛开源数据\ntrain = pd.read_csv(\"/kaggle/input/llm-detect-data/train1.csv\", sep=',')\n\n#https://www.kaggle.com/competitions/llm-detect-ai-generated-text\ntest = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\nsub = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\n\n\n\ntrain['text'] = train['text'].str.strip().replace('\\n', '')\ntest['text'] = test['text'].str.strip().replace('\\n', '')\n\ntrain = train.drop_duplicates(subset=['text'])\ntrain.reset_index(drop=True, inplace=True)\n\ntrain['label'].value_counts()\n\nVALID_MODE = len(test)==3\n#验证数据集\nif VALID_MODE:\n    valid = pd.read_csv('/kaggle/input/llm-detect-ai-validation2/nonTargetText_llm_slightly_modified_gen.csv')\n    valid = valid.dropna().reset_index(drop=True)\n    valid['text'] = valid['text'].str.strip().replace('\\n', '')\n    print(valid.shape)\nelse:\n    valid = pd.read_csv('/kaggle/input/llm-detect-ai-validation2/nonTargetText_llm_slightly_modified_gen.csv')\n    valid = valid.dropna().reset_index(drop=True)\n    valid['text'] = valid['text'].str.strip().replace('\\n', '')\n    print(valid.shape)\n    #train = pd.concat([train, valid]).drop_duplicates(subset=['text']).reset_index(drop=True, inplace=False)\n\nLOWERCASE = False\nVOCAB_SIZE = 4000 \n\n# Creating Byte-Pair Encoding tokenizer\nraw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\",))\n\n\n# Adding normalization and pre_tokenizer\nraw_tokenizer.normalizer = normalizers.Sequence([NFD(), \n])\nraw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n\n# Adding special tokens and creating trainer instance\nspecial_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\ntrainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens, )\n\n\n\n# Creating huggingface dataset object\nif VALID_MODE:\n    dataset = Dataset.from_pandas(valid[['text']])\nelse:\n    dataset = Dataset.from_pandas(test[['text']])\n\ndef train_corp_iter():\n    \"\"\"\n    A generator function for iterating over a dataset in chunks.\n    \"\"\"    \n    for i in range(0, len(dataset), 1000):\n        yield dataset[i : i + 1000][\"text\"]\n\n# Training from iterator REMEMBER it's training on test set...\nraw_tokenizer.train_from_iterator(train_corp_iter(), trainer=trainer)\nraw_tokenizer.model.save('.')\n\ntokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=raw_tokenizer,\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\",\n    sep_token=\"[SEP]\",\n    mask_token=\"[MASK]\",\n)\n\n\ntokenized_texts_test = []\n\n# Tokenize test set with new tokenizer\nfor text in tqdm(test['text'].tolist()):\n    tokenized_texts_test.append(tokenizer.tokenize(text, add_special_tokens=False))\n\n\n# Tokenize train set\ntokenized_texts_train = []\n\nfor text in tqdm(train['text'].tolist()):\n    tokenized_texts_train.append(tokenizer.tokenize(text, add_special_tokens=False))\n\nif VALID_MODE: \n    tokenized_texts_valid = []\n\n    for text in tqdm(valid['text'].tolist()):\n        tokenized_texts_valid.append(tokenizer.tokenize(text, add_special_tokens=False))\n    tokenized_texts_valid_aug = []\n    for text in tokenized_texts_valid:\n        tokenized_texts_valid_aug.append(text+text[::2]+text[1::2])\n\ndef dummy(text):\n    \"\"\"\n    A dummy function to use as tokenizer for TfidfVectorizer. It returns the text as it is since we already tokenized it.\n    \"\"\"\n    return text\n\ntokenized_texts_test_aug = []\nfor text in tokenized_texts_test:\n    tokenized_texts_test_aug.append(text+text[::2]+text[1::2])\n\ntokenized_texts_train_aug = []\nfor text in tokenized_texts_train:\n    tokenized_texts_train_aug.append(text+text[::2]+text[1::2])\n\n# Fitting TfidfVectoizer on test set\nmin_df = 2\nvectorizer = TfidfVectorizer(ngram_range=(3, 5), \n                             lowercase=False, \n                             sublinear_tf=True, \n                             analyzer = 'word',\n                            tokenizer = dummy,\n                            preprocessor = dummy,\n                            min_df = 2,\n                            token_pattern = None, \n                            strip_accents='unicode'\n                                                        )\nif VALID_MODE: \n    vectorizer.fit(tokenized_texts_valid_aug)\nelse:\n    vectorizer.fit(tokenized_texts_test_aug)\n\n# Getting vocab\nvocab = vectorizer.vocabulary_\n# Here we fit our vectorizer on train set but this time we use vocabulary from test fit.\nvectorizer = TfidfVectorizer(ngram_range=(3, 5), \n                             lowercase=False, \n                             sublinear_tf=True, \n                             vocabulary=vocab,\n                             min_df = 2,\n                            analyzer = 'word',\n                            tokenizer = dummy,\n                            preprocessor = dummy,\n                            token_pattern = None, strip_accents='unicode'\n                            )\n\ntf_train = vectorizer.fit_transform(tokenized_texts_train_aug)\ntf_test = vectorizer.transform(tokenized_texts_test_aug)\n\ny_train = train['label'].values\n\nbayes_model = MultinomialNB(alpha=0.023)\nsgd_model = SGDClassifier(max_iter=35000, tol=1e-4, loss=\"modified_huber\")\n\nweights = [0.2,0.8,]\n \nensemble = VotingClassifier(estimators=[('mnb',bayes_model),\n                                        ('sgd', sgd_model),\n                                       ],\n                            weights=weights, voting='soft', n_jobs=-1)\nensemble.fit(tf_train, y_train)\ngc.collect()\n\nfinal_preds = ensemble.predict_proba(tf_test)[:,1]\n\n#sub['generated'] = final_preds\n#sub.to_csv('sub_linear.csv', index=False)\n#sub\n\nif VALID_MODE:\n    tf_valid = vectorizer.transform(tokenized_texts_valid_aug)\n    y_pred = ensemble.predict_proba(tf_valid)[:,1]\n    from sklearn.metrics import roc_auc_score\n    print(\"* valid AUC-ROC score:\",roc_auc_score(valid[\"label\"], y_pred))\n    low_rank_index = rankdata(y_pred)<0.1*len(y_pred)\n    high_rank_index = rankdata(y_pred)>0.9*len(y_pred)\n    tf_train = vstack([tf_train, tf_valid[low_rank_index], tf_valid[high_rank_index]])\n    y_train = np.concatenate([y_train, sum(low_rank_index)*[0], sum(high_rank_index)*[1]])\n    bayes_model = MultinomialNB(alpha=0.02)\n    sgd_model = SGDClassifier(max_iter=25000, tol=1e-4, loss=\"modified_huber\")\n\n    weights = [0.5,0.5,]\n\n    ensemble2 = VotingClassifier(estimators=[('mnb',bayes_model),\n                                            ('sgd', sgd_model),\n                                           ],\n                                weights=weights, voting='soft', n_jobs=-1)\n    ensemble2.fit(tf_train, y_train)\n    y_pred = ensemble2.predict_proba(tf_valid)[:,1]\n    print(\"* pl valid AUC-ROC score:\",roc_auc_score(valid[\"label\"], y_pred))\n    final_preds2 = ensemble2.predict_proba(tf_test)[:,1]\n    sub['generated'] = final_preds2\n    sub.to_csv('sub_linear.csv', index=False)\n    sub\nelse:\n    low_rank_index = rankdata(final_preds)<0.1*len(final_preds)\n    high_rank_index = rankdata(final_preds)>0.9*len(final_preds)\n    \n    tf_train = vstack([tf_train, tf_test[low_rank_index], tf_test[high_rank_index]])\n    y_train = np.concatenate([y_train, sum(low_rank_index)*[0], sum(high_rank_index)*[1]])\n    bayes_model = MultinomialNB(alpha=0.02)\n    sgd_model = SGDClassifier(max_iter=25000, tol=1e-4, loss=\"modified_huber\")\n\n    weights = [0.5,0.5,]\n\n    ensemble2 = VotingClassifier(estimators=[('mnb',bayes_model),\n                                            ('sgd', sgd_model),\n                                           ],\n                                weights=weights, voting='soft', n_jobs=-1)\n    ensemble2.fit(tf_train, y_train)\n    final_preds2 = ensemble2.predict_proba(tf_test)[:,1]\n    sub['generated'] = final_preds2\n    sub.to_csv('sub_linear.csv', index=False)\n    sub","metadata":{"execution":{"iopub.status.busy":"2024-01-22T07:29:03.351878Z","iopub.execute_input":"2024-01-22T07:29:03.352259Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python lin_infer.py","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 基于语言模型的文本二分类模型：\n\n* 训练参考开源代码：https://www.kaggle.com/code/mustafakeser4/train-detectai-distilroberta-0-927","metadata":{}},{"cell_type":"code","source":"%%writefile distilroberta_infer.py#推理代码\nimport transformers\nimport datasets\nimport pandas as pd\nimport numpy as np\nfrom datasets import Dataset\nimport os\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport torch\nfrom transformers import AutoTokenizer\n##https://www.kaggle.com/datasets/mustafakeser4/detect-llm-models/versions/9\nmodel_checkpoint = \"/kaggle/input/detect-llm-models/distilroberta-finetuned_v5/checkpoint-13542\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\ndef preprocess_function(examples):\n    return tokenizer(examples['text'], max_length = 512 , padding=True, truncation=True)\nnum_labels = 2\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    # Move your model and data to the GPU\nmodel.to(device);\ntrainer = Trainer(\n    model,\n    tokenizer=tokenizer,\n)\ntest = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\ntest_ds = Dataset.from_pandas(test)\ntest_ds_enc = test_ds.map(preprocess_function, batched=True)\ntest_preds = trainer.predict(test_ds_enc)\nlogits = test_preds.predictions\nprobs = (np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True))[:,0]\nsub = pd.DataFrame()\nsub['id'] = test['id']\nsub['generated'] = probs\nsub.to_csv('sub_nn.csv', index=False)\nsub.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python distilroberta_infer.py","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 基于大规模数据的语言模型二分类：\n\n* 数据链接：https://www.kaggle.com/datasets/canming/piles-and-ultra-data\n    * 此数据来自huggingface作者开源，主要由以下数据集构成：\n    </br>https://huggingface.co/datasets/EleutherAI/the_pile_deduplicated\n    </br>https://huggingface.co/datasets/openbmb/UltraFeedback\n    </br>https://huggingface.co/datasets/stingning/ultrachat\n    </br>https://huggingface.co/datasets/lmsys/lmsys-chat-1m\n    </br>数据移除相似度大于0.8\n* 训练代码：deberta_train_exp5.py","metadata":{}},{"cell_type":"code","source":"%%writefile deberta_infer.py#推理代码\nimport transformers\nimport datasets\nimport pandas as pd\nimport numpy as np\nfrom datasets import Dataset\nimport os\nfrom transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\nimport torch\nfrom transformers import AutoTokenizer\n\nmodel_checkpoint = \"/kaggle/input/llm-diverse-model2/LLM7/0120/checkpoint-369284\"\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\ndef preprocess_function(examples):\n    return tokenizer(examples['text'], max_length = 512 , padding=True, truncation=True)\nnum_labels = 2\nmodel = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    # Move your model and data to the GPU\nmodel.to(device);\ntrainer = Trainer(\n    model,\n    tokenizer=tokenizer,\n)\ntest = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\ntest_ds = Dataset.from_pandas(test)\ntest_ds_enc = test_ds.map(preprocess_function, batched=True)\ntest_preds = trainer.predict(test_ds_enc)\nlogits = test_preds.predictions\nprobs = (np.exp(logits) / np.sum(np.exp(logits), axis=-1, keepdims=True))[:,1]\nsub = pd.DataFrame()\nsub['id'] = test['id']\nsub['generated'] = probs\nsub.to_csv('sub_nn2.csv', index=False)\nsub.head()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python deberta_infer.py","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nfrom scipy.stats import rankdata\np1=pd.read_csv('./sub_linear.csv').sort_values(['id']).reset_index(drop=True)\nnumber_sample = p1.shape[0]\np1['generated'] = rankdata(p1['generated'])/number_sample\n\np3=pd.read_csv('./sub_nn.csv').sort_values(['id']).reset_index(drop=True)\np3['generated'] = rankdata(p3['generated'])/number_sample\n\np4=pd.read_csv('./sub_nn2.csv').sort_values(['id']).reset_index(drop=True)\np4['generated'] = rankdata(p4['generated'])/number_sample\n\n\np1['generated'] = p1['generated']*0.7+p3['generated']*0.2+p4['generated']*0.1\np1[['id', 'generated']].to_csv('sub_stage1.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 二阶段的伪标签线性分类器","metadata":{}},{"cell_type":"code","source":"%%writefile lin_infer_stage2.py\nimport sys\nimport gc\nfrom scipy.sparse import vstack\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nfrom sklearn.metrics import roc_auc_score\nimport numpy as np\nimport glob\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom scipy.stats import rankdata\nfrom tokenizers import (\n    decoders,\n    models,\n    normalizers,\n    pre_tokenizers,\n    processors,\n    trainers,\n    Tokenizer,\n)\n\nfrom datasets import Dataset\nfrom tqdm.auto import tqdm\nfrom transformers import PreTrainedTokenizerFast\nfrom tokenizers.normalizers import (Sequence, Lowercase, NFD, \n                                   StripAccents)\n\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.ensemble import VotingClassifier\n\ntest = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/test_essays.csv')\nsub = pd.read_csv('/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv')\ntrain = pd.read_csv(\"/kaggle/input/daigt-v2-train-dataset/train_v2_drcat_02.csv\", sep=',')\ntrain = train[~train['source'].isin(['mistral7binstruct_v2', 'mistral7binstruct_v1','falcon_180b_v1','llama2_chat', 'llama_70b_v1', 'NousResearch/Llama-2-7b-chat-hf'])].reset_index(drop=True)\nextra = pd.read_csv('/kaggle/input/argugpt/argugpt.csv')\nextra['label'] = 1\nextra = extra[['text', 'label']]\n\nextra2 = pd.read_csv('/kaggle/input/llm-detect-sim-filter-daigt-v3/llama_filter.csv')\nextra3 = pd.read_csv('/kaggle/input/llm-detect-sim-filter-daigt-v3/falcon_filter.csv')\n\nextra5 = pd.read_csv('/kaggle/input/llm-detect-sim-filter-essayfroum/essayforum_writingt_filter.csv')\n\nextra6 = pd.read_csv('/kaggle/input/llm-detect-sim-filter-daigt-v3/mistral_filter.csv')\n\nextra7 = pd.read_csv('/kaggle/input/llm-detect-sim-filter-nahedabdelgaber/evaluating-student-writing_filter.csv')\nextra7 = extra7.sample(8000, random_state=100).reset_index(drop=True)\nextra7['label'] = 1\nextra7 = extra7[['text', 'label']]\n\ntrain = pd.concat([train, extra2, extra3, extra5, extra6, extra7], ignore_index=True)\n#if len(test)==3:\n#    train = train.sample(1000, random_state=100).reset_index(drop=True)\ntrain['text'] = train['text'].str.strip().replace('\\n', '')\ntest['text'] = test['text'].str.strip().replace('\\n', '')\n\ntrain = train.drop_duplicates(subset=['text'])\ntrain.reset_index(drop=True, inplace=True)\n\ntrain['label'].value_counts()\n\nVALID_MODE = len(test)==3\n\nif VALID_MODE:\n    valid = pd.read_csv('/kaggle/input/llm-detect-ai-validation2/nonTargetText_llm_slightly_modified_gen.csv')\n    valid = valid.dropna().reset_index(drop=True)\n    valid['text'] = valid['text'].str.strip().replace('\\n', '')\n    print(valid.shape)\nelse:\n    valid = pd.read_csv('/kaggle/input/llm-detect-ai-validation2/nonTargetText_llm_slightly_modified_gen.csv')\n    valid = valid.dropna().reset_index(drop=True)\n    valid['text'] = valid['text'].str.strip().replace('\\n', '')\n    print(valid.shape)\n    #train = pd.concat([train, valid]).drop_duplicates(subset=['text']).reset_index(drop=True, inplace=False)\n\nLOWERCASE = False\nVOCAB_SIZE = 3500 \n\n# Creating Byte-Pair Encoding tokenizer\nraw_tokenizer = Tokenizer(models.BPE(unk_token=\"[UNK]\",))\n\n\n# Adding normalization and pre_tokenizer\nraw_tokenizer.normalizer = normalizers.Sequence([NFD(), \n])\nraw_tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()\n\n# Adding special tokens and creating trainer instance\nspecial_tokens = [\"[UNK]\", \"[PAD]\", \"[CLS]\", \"[SEP]\", \"[MASK]\"]\ntrainer = trainers.BpeTrainer(vocab_size=VOCAB_SIZE, special_tokens=special_tokens, )\n\n\n\n# Creating huggingface dataset object\nif VALID_MODE:\n    dataset = Dataset.from_pandas(valid[['text']])\nelse:\n    dataset = Dataset.from_pandas(test[['text']])\n\ndef train_corp_iter():\n    \"\"\"\n    A generator function for iterating over a dataset in chunks.\n    \"\"\"    \n    for i in range(0, len(dataset), 1000):\n        yield dataset[i : i + 1000][\"text\"]\n\n# Training from iterator REMEMBER it's training on test set...\nraw_tokenizer.train_from_iterator(train_corp_iter(), trainer=trainer)\nraw_tokenizer.model.save('.')\n\ntokenizer = PreTrainedTokenizerFast(\n    tokenizer_object=raw_tokenizer,\n    unk_token=\"[UNK]\",\n    pad_token=\"[PAD]\",\n    cls_token=\"[CLS]\",\n    sep_token=\"[SEP]\",\n    mask_token=\"[MASK]\",\n)\n\n\ntokenized_texts_test = []\n\n# Tokenize test set with new tokenizer\nfor text in tqdm(test['text'].tolist()):\n    tokenized_texts_test.append(tokenizer.tokenize(text, add_special_tokens=False))\n\n\n# Tokenize train set\ntokenized_texts_train = []\n\nfor text in tqdm(train['text'].tolist()):\n    tokenized_texts_train.append(tokenizer.tokenize(text, add_special_tokens=False))\n\nif VALID_MODE: \n    tokenized_texts_valid = []\n\n    for text in tqdm(valid['text'].tolist()):\n        tokenized_texts_valid.append(tokenizer.tokenize(text, add_special_tokens=False))\n    tokenized_texts_valid_aug = []\n    for text in tokenized_texts_valid:\n        tokenized_texts_valid_aug.append(text+text[::2]+text[1::2])\n\ndef dummy(text):\n    \"\"\"\n    A dummy function to use as tokenizer for TfidfVectorizer. It returns the text as it is since we already tokenized it.\n    \"\"\"\n    return text\n\ntokenized_texts_test_aug = []\nfor text in tokenized_texts_test:\n    tokenized_texts_test_aug.append(text+text[::2]+text[1::2])\n\ntokenized_texts_train_aug = []\nfor text in tokenized_texts_train:\n    tokenized_texts_train_aug.append(text+text[::2]+text[1::2])\n\n# Fitting TfidfVectoizer on test set\nmin_df = 2\nvectorizer = TfidfVectorizer(ngram_range=(3, 5), \n                             lowercase=False, \n                             sublinear_tf=True, \n                             analyzer = 'word',\n                            tokenizer = dummy,\n                            preprocessor = dummy,\n                            min_df = 2,\n                            token_pattern = None, \n                            strip_accents='unicode'\n                                                        )\nif VALID_MODE: \n    vectorizer.fit(tokenized_texts_valid_aug)\nelse:\n    vectorizer.fit(tokenized_texts_test_aug)\n\n# Getting vocab\nvocab = vectorizer.vocabulary_\n# Here we fit our vectorizer on train set but this time we use vocabulary from test fit.\nvectorizer = TfidfVectorizer(ngram_range=(3, 5), \n                             lowercase=False, \n                             sublinear_tf=True, \n                             vocabulary=vocab,\n                             min_df = 2,\n                            analyzer = 'word',\n                            tokenizer = dummy,\n                            preprocessor = dummy,\n                            token_pattern = None, strip_accents='unicode'\n                            )\n\ntf_train = vectorizer.fit_transform(tokenized_texts_train_aug)\ntf_test = vectorizer.transform(tokenized_texts_test_aug)\n\ny_train = train['label'].values\n\nbayes_model = MultinomialNB(alpha=0.02)\nsgd_model = SGDClassifier(max_iter=35000, tol=1e-4, loss=\"modified_huber\")\n\nweights = [0.1,0.9,]\n \nensemble = VotingClassifier(estimators=[('mnb',bayes_model),\n                                        ('sgd', sgd_model),\n                                       ],\n                            weights=weights, voting='soft', n_jobs=-1)\nensemble.fit(tf_train, y_train)\ngc.collect()\nstage1 = pd.read_csv('sub_stage1.csv')\nfinal_preds = stage1['generated'].values\n\n#sub['generated'] = final_preds\n#sub.to_csv('sub_linear.csv', index=False)\n#sub\n\nif VALID_MODE:\n    tf_valid = vectorizer.transform(tokenized_texts_valid_aug)\n    y_pred = ensemble.predict_proba(tf_valid)[:,1]\n    from sklearn.metrics import roc_auc_score\n    print(\"* valid AUC-ROC score:\",roc_auc_score(valid[\"label\"], y_pred))\n    low_rank_index = rankdata(y_pred)<0.1*len(y_pred)\n    high_rank_index = rankdata(y_pred)>0.9*len(y_pred)\n    tf_train = vstack([tf_train, tf_valid[low_rank_index], tf_valid[high_rank_index]])\n    y_train = np.concatenate([y_train, sum(low_rank_index)*[0], sum(high_rank_index)*[1]])\n    bayes_model = MultinomialNB(alpha=0.02)\n    sgd_model = SGDClassifier(max_iter=25000, tol=1e-4, loss=\"modified_huber\")\n\n    weights = [0.5,0.5,]\n\n    ensemble2 = VotingClassifier(estimators=[('mnb',bayes_model),\n                                            ('sgd', sgd_model),\n                                           ],\n                                weights=weights, voting='soft', n_jobs=-1)\n    ensemble2.fit(tf_train, y_train)\n    y_pred = ensemble2.predict_proba(tf_valid)[:,1]\n    print(\"* pl valid AUC-ROC score:\",roc_auc_score(valid[\"label\"], y_pred))\n    final_preds2 = ensemble2.predict_proba(tf_test)[:,1]\n    sub['generated'] = final_preds2\n    sub.to_csv('sub_linear_stage2.csv', index=False)\n    sub\nelse:\n    low_rank_index = rankdata(final_preds)<0.15*len(final_preds)\n    high_rank_index = rankdata(final_preds)>0.85*len(final_preds)\n    \n    tf_train = vstack([tf_train, tf_test[low_rank_index], tf_test[high_rank_index]])\n    y_train = np.concatenate([y_train, sum(low_rank_index)*[0], sum(high_rank_index)*[1]])\n    bayes_model = MultinomialNB(alpha=0.022)\n    sgd_model = SGDClassifier(max_iter=33000, tol=1e-4, loss=\"modified_huber\")\n\n    weights = [0.1,0.9,]\n\n    ensemble2 = VotingClassifier(estimators=[('mnb',bayes_model),\n                                            ('sgd', sgd_model),\n                                           ],\n                                weights=weights, voting='soft', n_jobs=-1)\n    ensemble2.fit(tf_train, y_train)\n    final_preds2 = ensemble2.predict_proba(tf_test)[:,1]\n    sub['generated'] = final_preds2\n    sub.to_csv('sub_linear_stage2.csv', index=False)\n    sub","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!python lin_infer_stage2.py","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 二阶段融合","metadata":{}},{"cell_type":"code","source":"import os\nimport random\nimport numpy as np\nimport tensorflow as tf\nimport pandas as pd\nfrom scipy.stats import rankdata\np1=pd.read_csv('./sub_linear_stage2.csv').sort_values(['id']).reset_index(drop=True)\nnumber_sample = p1.shape[0]\np1['generated'] = rankdata(p1['generated'])/number_sample\n\np3=pd.read_csv('./sub_nn.csv').sort_values(['id']).reset_index(drop=True)\np3['generated'] = rankdata(p3['generated'])/number_sample\n\np4=pd.read_csv('./sub_nn2.csv').sort_values(['id']).reset_index(drop=True)\np4['generated'] = rankdata(p4['generated'])/number_sample\n\n\np1['generated'] = p1['generated']*0.7+p3['generated']*0.2+p4['generated']*0.1\np1[['id', 'generated']].to_csv('submission.csv', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}